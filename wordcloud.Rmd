---
title: Data exploration
output: html_notebook
---
Uploading packages and cleaned data. Extracting the text from reviews:

```{r}
library(tidytext)
library(tidyverse)
library(wordcloud)
library(RColorBrewer)
library(tibble)

text_extracted <- cleaned_data %>% select(content)
text_extracted <- text_extracted %>% rename(text = content) 
```
with the help of `mutate()` function, we annotate the data by `linenumber` to keep track of lines:
```{r}
annotatedData <- text_extracted  %>%  
                mutate(linenumber = row_number())
```
we tokenize the texts to restructure the data into _one-token-per-row_ format, for which we first ungroup the `annotatedData` and pass it to the `unnest_tokens()` function, as shown below:
```{r} 
organized_text <- annotatedData %>% ungroup
tidy_text <- organized_text %>% unnest_tokens(word, text)
```
Removing the stop words of SMART lexicon: 
```{r}
tidy_text %>% anti_join(get_stopwords(source="smart"))
clean_tokens <- tidy_text %>% anti_join(stop_words)
```
Sorting the tokens and have a look at the most frequently used words in our text corpus:
```{r}
distribution_words <- clean_tokens %>% count(word, sort = TRUE)
distribution_words
```
Visualizing them as a bar chart:
```{r}
words_plot <- distribution_words %>%
  filter(n > 5301 ) %>%   # only words with frequency > 5301
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  coord_flip() +
  geom_col(show.legend = FALSE, fill="orange") +
  labs(y= "Most popular words from all reviews")
words_plot
```
Creating a colorful wordcloud:
```{r}
pal <- brewer.pal(8,"Dark2")

wordcloud(distribution_words$word, distribution_words$n, max.words= 40, random.order = FALSE, colors=pal)
```