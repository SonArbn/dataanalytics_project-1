---
title: "Dating apps"
output: html_notebook
---

Average score over time:
```{r}
library(tidyverse)
library(fpp3)
library(ggplot2)

#timeseries (create a subset so I can produce plots)

timeseries <-  dating %>% mutate(Month = yearmonth(Date)) %>% select(score, Month, Group) %>% group_by(Month)


# average score per month all apps

timeseries2 <- timeseries %>% summarize(average_score = mean(score)) %>% group_by(Month)
timeseries2 %>% ggplot(mapping = aes(x=Month, y=average_score)) + geom_line() + labs(titel = "Average score from 2010 to 2022 of dating Apps", x = "Time (Month)", y = "Average Score")

#Amount of reviews per year
timeseries2.0 <- dating %>% mutate(Year = year(Date)) %>% select(score, Year, Group, content) %>% group_by(Year)

#Here I had to find out how many reviews were done in a year
T2010 <- timeseries2.0 %>% filter(Year == 2010)
T2011 <- timeseries2.0 %>% filter(Year == 2011)
T2012 <- timeseries2.0 %>% filter(Year == 2012)
T2013 <- timeseries2.0 %>% filter(Year == 2013)
T2014 <- timeseries2.0 %>% filter(Year == 2014)
T2015 <- timeseries2.0 %>% filter(Year == 2015)
T2016 <- timeseries2.0 %>% filter(Year == 2016)
T2017 <- timeseries2.0 %>% filter(Year == 2017)
T2018 <- timeseries2.0 %>% filter(Year == 2018)
T2019 <- timeseries2.0 %>% filter(Year == 2019)
T2020 <- timeseries2.0 %>% filter(Year == 2020)
T2021 <- timeseries2.0 %>% filter(Year == 2021)
T2022 <- timeseries2.0 %>% filter(Year == 2022)

#then I created a data frame with each year and the amount of reviews every year

df1 <- data.frame(Year = c('2010','2011','2012','2013','2014','2015','2016','2017','2018','2019','2020','2021','2022'), 
                  Reviews = c(337,2292,6773,16734,51999,69656,78941,84124,95923,151928,124497,106499,28012))

#finally I made a plot
df1 %>% ggplot(mapping = aes(x=Year, y=Reviews)) + geom_col() + labs(titel = "Number of Reviews per Year", x = "Year", y = "Amount of Reviews")


```
We produced two plots in advance. The first one shows that users scored the dating apps worse in the last years than in the beginning. A crucial point happended in 2020 when a clear downwards trend is seen.
The second plot shows that the amount of reviews given over time increased. So more people comment and more negative scores are given. 


```{r}
library(tidytext)
library(tidyverse)
library(tibble)
library(wordcloud)
library(RColorBrewer)
library(tibble)
```

Loading the datasets, organizing columns and merging into one data set: 
```{r}
tinder <- read.csv(file = "tinder.csv")

okcupid <- read.csv(file = "okcupid.csv")

bumble <- read.csv(file = "bumble.csv")

hinge <- read.csv(file = "hinge.csv")
  
filter_data <- function(mydata){
  mydata <- select(mydata, userName, content, score, thumbsUpCount,reviewCreatedVersion, at, replyContent)
  mydata <- filter(mydata, mydata$at > as.POSIXct("2020-01-01 00:00:00", tz="UTC")) 
  filter(mydata, thumbsUpCount != 0 )
  }

tinder <- filter_data(tinder)
appName <- rep(c("Tinder"))
tinder <- cbind(tinder, appName)

okcupid <- filter_data(okcupid)
appName <- rep(c("okcupid"))
okcupid <- cbind(okcupid, appName)

bumble <- filter_data(bumble)
appName <- rep(c("bumble"))
bumble <- cbind(bumble, appName)

hinge <- filter_data(hinge)
appName <- rep(c("hinge"))
hinge <- cbind(hinge, appName)

all_reviews <- rbind(tinder,okcupid,bumble,hinge)
```
Calculating mean score for each app:
```{r}
t_meanscore <- mean(tinder$score)
ok_meanscore <- mean(okcupid$score)
b_meanscore <- mean(bumble$score)
h_meanscore <- mean(hinge$score)
```
Creating subsets for each score:
```{r}
score_1 <- filter(all_reviews, score == 1 )
score_2 <- filter(all_reviews, score == 2 )
score_3 <- filter(all_reviews, score == 3 )
score_4 <- filter(all_reviews, score == 4 )
score_5 <- filter(all_reviews, score == 5 )

```
```{r}
text_1 <- score_1 %>% select(content)
text_1 <- text_1 %>% rename(text = content) 
```
with the help of `mutate()` function, we annotate the data by `linenumber` to keep track of lines:
```{r}
annotatedData <- text_1  %>%  
                mutate(linenumber = row_number())
```
we tokenize the texts to restructure the data into _one-token-per-row_ format, for which we first ungroup the `annotatedData` and pass it to the `unnest_tokens()` function, as shown below:
```{r} 
organized_text <- annotatedData %>% ungroup
tidy_text <- organized_text %>% unnest_tokens(bigram, text, token = "ngrams", n = 2)
tidy_text <- tidy_text %>% count(bigram, sort = TRUE)
bigrams_separated <- tidy_text %>% separate(bigram, c("word1", "word2"), sep = " ")
```
Filter out those rows in which either `word1` or `word2` is a stop word:
```{r}
bigram_filtered <- bigrams_separated %>% 
  filter(!word1 %in% stop_words$word & !word2 %in% stop_words$word)
```
Removing the stop words of SMART lexicon: 
```{r}
bigram_cleaned <- bigram_filtered %>% anti_join(get_stopwords(source="smart"))
```
Sorting the tokens and have a look at the most frequently used words in our text corpus:
```{r}
distribution_words <- bigram_filtered %>% count(word, sort = TRUE)
bigrams <- bigram_filtered %>% 
            unite(bigram, word1, word2, sep=" ")
bigrams %>% count(bigram, sort=TRUE)
```
Creating custom stopwords:
```{r}
stop_words <- c("dating app","dating apps","worst app")
bigrams <- anti_join(get_stopwords(source="smart"), stop_words)
```
Visualizing them as a bar chart:
```{r}
tokenized_counts = bigrams %>% 
  count(word) %>% 
  filter(n > 100) # eliminate infrequent words

# iterate through words and calc correlations

correlations = numeric(nrow(bigrams))
names(correlations) = bigrams$bigram
for(i in 1:nrow(bigrams)){
  print(i)
  bigram_i = bigrams %>% slice(i) %>% pull(bigram)
  bigram_present = str_detect(bigrams, bigram_i)
  correlations[i] = cor(bigram_present, score_1$score)  
  }
```
Visualizing:
```{r}
bigram_plot <- bigrams %>%
  filter(n > 172 ) %>%   # only bigrams with frequency > 172
  mutate(word = reorder(bigram, n)) %>%
  ggplot(aes(bigram, n)) +
  geom_col() +
  coord_flip() +
  geom_col(show.legend = FALSE, fill="orange") +
  labs(y= "Most popular bigrams from worst reviews (1 score)")
bigram_plot

bigrams %>% bigrams$filter(str_contains('app', bigram))

```
Creating a colorful wordcloud:
```{r}
pal <- brewer.pal(8,"Dark2")

wordcloud(bigrams$n, max.words= 40, random.order = FALSE, colors=pal)
```
```{r}
saveRDS(result, file = "cleaned_data.rds")
```
